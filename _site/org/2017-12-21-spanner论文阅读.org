#+BEGIN_HTML
---
layout: post
title: spanner 论文阅读
---
#+END_HTML
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil

* spanner 是什么
  google 开发的一个数据库。用于支持 scheme 在线变更，避免 msyql 的手工 sharding 及支持自动 failover。feature:
   - 可拓展
   - 多版本
   - 全球分布式
   - 同步复制

* spanner 的系统结构
  - 一个部署的 spanner 称为 universe, 平行宇宙的意思？
  - 部署单元是 zone, zone 里面的数据可以相互复制, zone 还是物理隔离的单元，同一个机房的机器可以划分成不同 zone
  - zone 里面的节点角色分为两个：zonemaster 和 spanserver
     - zonemaster 负责分发数据给 spanserver
     - spanserver 对外提供数据服务
  - placement driver 负责定时查询 spanserver，然后做数据搬移

** spanserver 的软件结构
   - spanserver 的底层存储的抽象是 tablet，一个 spanserver 负责 100-1000 个 tablet
   - tablet 的概念是一张表中的某些行
   - tablet 的结构是 set(structure）, 其中 structure = (key:string, timestamp:int64) -> string.
     - key 是由用户指定的 key column 组成的
     - timestamp 用于多版本控制
     - 最后的 string 是 row 中所有 column 组成的
   - tablet 的数据存在 google 的分布式系统 Colossus 中，结构是 b-tree-like 文件 和 write ahead log
   - 为了实现可复制，在 tablet 上使用了 paxos 状态机，paxos 状态机的 meta data 和 log 都存在 tablet 中
   - 多个由 paxos 保持一致性的副本组成 paxos group，paxos group 中会选出一个 leader, 对应的 spanserver 上实现了 lock table 以及 transactional manager
     - lock table 保存了 key range 到 lock state 的对应
     - transaction manager 用于实现 2pc. 涉及到多个 paxos group 的 transaction 会选择出一个 group 作为 coordinator leader, 其他 group 作为 coordinator slaves
   - 写操作先经过 paxos 协议，通过 paxos 保持强一致性。读操作可以在任意一台 tablet 足够新的 spanserver 上执行

* Truetime API
  | Method      | Returns                              |
  |-------------+--------------------------------------|
  | TT.now()    | TTintervals:[earlist, latest]        |
  | TT.after()  | true if t has definitely passed      |
  | TT.before() | true if t has definitely not arrived |
  - 使用原子钟和 GPS 保证机器之间的时间同步，平均误差保持在 10ms 以内
  - 使用同步的 wall time 去保证事务之间的外部一致性

* 并发控制
  联合 Truetime API，实现
  - externally consistent transactions
  - lockfree read-only transactions
  - non-blocking reads in the past

** 时间戳管理
   - 支持的事务类型
     - readwrite transactions
     - read-only transactions
     - snapshot reads
   - 多版本控制与时间戳的关系
     - 每个事务分配一个时间戳
     - 数据写入的时候带着一个时间戳

*** paxos leader 续约
    - paxos leader 的 leadership 持续时间是 10s,在 leadership 接近超时的时候发起续约请求，如果收到超过半数投票的话，成功续约。否则超时，选新 leader
    - 两个相继的 leader 之间的租约时间是不能重叠的，假设 $s_{max}$ 是一个 leader 租约时间的尽头，那么在 $s_{max}$ 之前 leader 不会退出

*** 为 RW transaction 分配时间戳
    - 时间线
      - client 发起读写事务
      - coordinator leader 收到消息之后，先申请写锁，然后发出 perpare 请求
      - coordinator participant 收到 prepare 请求，分配时间戳，记录 paxos log。将时间戳返回给 leader
        - coordinator participant 分配时间戳是单调递增的。因为 leader 选举时保证相互之间的租期是不重叠的，leader 只能分配自己租期内的时间戳，假设分配的最大时间戳是 $s_{max}$，那么 leader 要等到 $s_{max}$ 过去之后才能退出
      - coordinator leader 收到时间戳之后，计算出一个时间戳比报上来的所有时间戳都要大，假设为 s
      - coordinator leader 等到 TT.time.after(s) = true 之后，发送 commit 请求。等待的时间是 commit wait
      - coordinator participant 收到 commit 消息，将 paxos log 的结果 apply 到实际存储中

    - 外部一致性：
      - 事务2开始时，事务1已经提交了，那么分配的时间戳要满足是：$s_2 > s_1$
      - 外部一致性的含义是：transaction 的 commit 顺序跟用户可以看到事务执行的顺序是一样的，比如用户先发起事务1然后发起事务2，那么他能看到的结果是事务1执行的基础上，事务2再执行的结果。这个顺序使得分布式的数据库跟单台数据库看起来是一样的
      - 得益于 Truetime API 提供的全球统一授时，commit 的 timestamp 用 global wall-clock time 顺序就满足了这个性质，timestamp order == commit order. 用真实世界的时间去给事务发生的先后顺序排序

*** 分配读请求的时间戳
    - 一个 replica 能够提供读服务的最大时间戳记为 $t_{safe}$, 那么时间戳 t 的请求只要满足 $t<=t_{safe}$ 就可以由这个 replica 提供服务
    - $t_{safe}$ 的确定
      - $t_{safe}^{Paxos}$: 对于 paxos 安全的时间点，等于最后一个 apply 的 paxos 写的时间戳，因为 paxos 的时间戳是单调递增的
      - $t_{safe}^{TM}$: 对于事务安全的时间点。如果没有正在 prepared 但是还没有提交的 transaction，那么怎么读都不会读到别的 transaction 的数据，是安全的。如果有 prepared 但没有 commit 的 transaction，找出这个 transaction 将要 commit 的 lower bound 作为安全点，在两阶段提交的 prepare 阶段是有一个时间点的，coordinator leader 保证 $Timestamp(commit) >= Timestamp(prepare_g), \forall{g}$. 所以 $Timestamp(TM_{safe}) = \min(Timestamp(prepare_g)) - 1,\forall{g}$
      - $t_{safe}=\min(t_{safe}^{Paxos}, t_{safe}^{TM})$
